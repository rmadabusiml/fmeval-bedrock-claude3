# Evaluate Anthropic Claud 3 Models with AWS Bedrock & FMEval Custom Model Runner

## Introduction
Evaluating Large Language Models (LLMs) is essential to ensure they meet the specific needs of diverse use cases while maintaining responsible AI practices. This project delves into how <a href="https://aws.amazon.com/bedrock/" target="_blank">AWS Bedrock</a> and <a href="https://github.com/aws/fmeval" target="_blank">FMEval</a> facilitate the evaluation of Anthropic Claude 3 models. These tools provide a comprehensive framework for assessing model performance, helping data scientists and ML engineers to streamline their evaluation processes. The ability to conduct large-scale evaluations with built-in and custom algorithms makes FMEval an invaluable resource for those working with LLMs.

Anthropic’s Claude 3 models, including Claude 3 Opus, Claude 3 Sonnet, and Claude 3 Haiku, set new standards in reasoning, math, and coding. These models offer significant improvements in understanding non-English languages, maintaining high accuracy, and reducing hallucinations compared to other models. By leveraging AWS Bedrock and FMEval, users can evaluate these advanced models effectively, ensuring optimal performance and reliability.

## Getting Started with FMEval
FMEval can be used wherever your workloads run, as a Python package or via the open-source code repository on GitHub. It provides easy-to-understand data and reports, enabling AWS customers to make informed decisions. Users can upload their own prompt datasets and algorithms. The core evaluation function, evaluate(), is extensible, allowing users to upload a prompt dataset, select and upload an evaluation function, and run an evaluation job. Results are delivered in multiple formats, helping to review, analyze, and operationalize high-risk items, and choose the right LLM for specific use cases.

### Supported Algorithms
FMEval offers 12 built-in evaluations covering four different tasks and five evaluation dimensions. The library is designed with extensibility in mind, based on the latest scientific findings and the most popular open-source evaluations. The proposed evaluations are intended to cover popular aspects out-of-the-box and enable the addition of new ones.

#### Evaluation Tasks and Dimensions
- Open-ended generation: Evaluates prompt stereotyping, toxicity, factual knowledge, and semantic robustness.
- Text summarization: Measures accuracy, toxicity, and semantic robustness.
- Question answering (Q&A): Assesses accuracy, toxicity, and semantic robustness.
- Classification: Evaluates accuracy and semantic robustness.

#### Evaluation Details
- Accuracy: Assesses model performance across tasks such as summarization, Q&A, and classification, using metrics like ROUGE-N, METEOR, and BERTScore for summarization; Exact Match, Quasi-Exact Match, and F1 scores for Q&A; and standard metrics like accuracy, precision, recall, and balanced accuracy for classification.
- Semantic Robustness: Evaluates the performance change in model output due to semantic-preserving perturbations like typos, random upper-case changes, and whitespace modifications.
- Factual Knowledge: Measures the model’s ability to reproduce real-world facts using prompts from datasets like T-REx.
- Prompt Stereotyping: Analyzes whether the model encodes stereotypes across various categories, using the CrowS-Pairs dataset.
- Toxicity: Assesses the level of toxic content generated by the model, using datasets like Real Toxicity Prompts and BOLD, with UnitaryAI Detoxify-unbiased as the toxicity detector.

#### Using FMEval for Evaluations
The FMEval package includes core constructs necessary for conducting evaluation jobs:

- Data Config: Points to the dataset location and contains fields like model_input, target_output, and model_output. It can be customized based on the evaluation algorithm.
- Model Runner: Represents the hosted FM used for inference. FMEval supports native runners for JumpStart, Amazon Bedrock, and SageMaker endpoints, and allows for custom runners.
- Evaluation Algorithm: Supports built-in and custom algorithms for evaluating LLMs. Users can implement custom logic by inheriting the base Evaluation Algorithm class.

FMEval empowers users to evaluate their models comprehensively, providing insights to optimize performance and ensure responsible AI practices.

## FMEval Custom Claude 3 Model Runner
FMEval provides two model runners: SageMaker JumpStart & Bedrock out of the box. Custom model runners can be created by extending existing Model Runner classes that require special processing.Bedrock Model Runner uses <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html" target="_blank">Text Completion API</a> for older Anthropic Claude 2.x verison models but in order to evaluate newer Anthropic Claude 3 Models such as Haiku or Sonnet or Opus, a custom Model Runner need to be created as it uses <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html" target="_blank">Messages API</a>. 

This project focuses on how to create a custom Model Runner and evaluate Claude 3 Models on couple of use cases: Summarization & Factual Knowledge.

## Pre-requisite
```
python -m venv venv 
source venv/bin/activate
pip install -r requirements.txt
```
**Note:** python version >=3.10 is preferrable

## Custom Bedrock Model Runner for Claude 3

Take a look at bedrock_claud3_model_runner.py on the implementation of custom model runner.

## Summarization Use Case:
A typical Evaluation require a json file with .jsonl extension that contains few samples. Each sample contains a document key with some text as value, and summary key with expected output. Each sample is fed into the model and its output is evaluated with the expected summary and provides 3 different metrics such as METEOR, ROUGE and BERTScore.

```
# summarization_bedrock.py refers data/summarize_accuracy.jsonl file for samples
python summarization_bedrock.py
```

## Factual Knowledge Use Case:
A typical Evaluation require a json file with .jsonl extension that contains few samples. Each sample contains a question key with some text as value, answer key with expected output (It can have multiple answers with OR), and knowledge_category key to indicate category. Each sample is fed into the model and its output is evaluated with the expected summary and provides 1 metric score with either 0 or 1 for no-match or match respectively.

```
# factual_knowledge.py refers data/factual_knowledge.jsonl file for samples
python factual_knowledge.py
```